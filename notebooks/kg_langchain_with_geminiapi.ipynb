{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Install requirements"
      ],
      "metadata": {
        "id": "XdbuAJbv2Y9E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRsN9cGRwnXZ",
        "outputId": "8b3c145d-9eb4-4965-b993-feb26a38771d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m973.5/973.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.2/310.2 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.4/124.4 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --quiet  langchain-google-genai"
      ],
      "metadata": {
        "id": "_8WLdq_xwytN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU google-generativeai instructor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S24pTvAY5NKj",
        "outputId": "a6c21314-6f91-45f6-a656-297e6a2c0af9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m791.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU neo4j"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UhT9ih2JsMM",
        "outputId": "26eefbaa-03f0-4aad-d3af-4076b82d896a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/203.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/203.0 kB\u001b[0m \u001b[31m774.9 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/203.0 kB\u001b[0m \u001b[31m749.3 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m143.4/203.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.0/203.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for neo4j (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain_community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HntyViSCJyAQ",
        "outputId": "d4e05a63-14f1-47ed-e72f-126980e4d9f6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Pydantic"
      ],
      "metadata": {
        "id": "5vqkRIqL2dTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "# from typing import Dict, Any, List\n",
        "\n",
        "# class Node(BaseModel):\n",
        "#     name: str = Field(description=\"Name of entity, ex: job position, company name, etc\")\n",
        "#     attributes: Dict[str, str] = Field(description=\"Attribute of entity\")\n",
        "\n",
        "# class Edge(BaseModel):\n",
        "#     name: str = Field(description= \"Name of relationship between 2 entities\")\n",
        "#     source: Node = Field(description= \"Entity is subject in relationship\")\n",
        "#     target: Node = Field(description= \"Entity is object in relationship\")\n",
        "\n",
        "# class KnowledgeGraph(BaseModel):\n",
        "#     nodes: List[Node] = Field(default_factory=list)\n",
        "#     edges: List[Node] = Field(default_factory=list)"
      ],
      "metadata": {
        "id": "H4LpBC0-2cV5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"API_KEY\"] = \"AIzaSyCLPT6eXs818h7o3tLm8CUeJMHdKcKXPug\""
      ],
      "metadata": {
        "id": "IHs9iqpC5znH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "genai.configure(api_key=os.environ[\"API_KEY\"]) # alternative API key configuration"
      ],
      "metadata": {
        "id": "Kc5D3DgA5zbD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "job_description = \"\"\"\n",
        "KMS Technology was established in 2009 as a U.S.-based software services company. With development centers in Vietnam and Mexico, we have been trusted globally for the superlative quality of our software consulting & development services, technology solutions, and engineers' expertise. We pride ourselves on creating brilliant solutions for our clients by leveraging deep expertise, advanced technologies, and delivery excellence for a shared success where everyone can reach their fullest potential. With three Business Lines:\n",
        "\n",
        "KMS Software: Leverage software domain expertise to help clients make better business decisions in technology platforms, increase speed-to-market, and gain critical development support through innovative technology solutions.\n",
        "KMS Solutions: Empower BFSI businesses to embrace the digital finance revolution and expedite clients’ journey towards complete digitalization, technology consulting, data analytics, software development, and software quality.\n",
        "KMS Healthcare: Build transformative next-gen technologies to solve healthcare’s most challenging problems, providing innovative tools and expertise to providers, payers, life sciences, and medical technology vendors.\n",
        "Responsibilities\n",
        "About Innovation Center\n",
        "\n",
        "At KMS Innovation Center, we focus on pushing the boundaries of technology to create revolutionary solutions that anticipate and exceed the evolving needs of the industry. The Innovation Center is a hub of creativity and technological advancement, where you will have the opportunity to work alongside pioneers in AI and software development.\n",
        "\n",
        "Join Our Quest: AI/ML Engineer Internship Opportunity\n",
        "\n",
        "As an AI/ML Intern at the KMS Innovation Center, you will have the unique opportunity to work on the forefront of AI and emerging technologies. You will contribute to the development and evaluation of AI-driven tools and solutions, enhancing our software development lifecycle and meeting the dynamic needs of our customers.\n",
        "\n",
        "Responsibilities\n",
        "\n",
        "As an AI/ML Engineer Intern, you will:\n",
        "\n",
        "Explore and evaluate the latest advancements in AI models and emerging technologies.\n",
        "\n",
        "Collaborate on building proofs of concept for AI applications and solutions.\n",
        "\n",
        "Work closely with technical leaders and skilled developers, adhering to high-standard development practices.\n",
        "\n",
        "Contribute to innovative solutions tailored to the needs of our customers and the broader industry.\n",
        "\n",
        "Engage in continuous learning activities to stay current with industry trends and advancements in AI/ML.\n",
        "\n",
        "Qualifications\n",
        "Pursuing a degree in Computer Science, AI/ML, or a related field.\n",
        "\n",
        "Solid understanding of NLP, Deep Learning, and Generative AI, with practical experience in machine learning frameworks such as TensorFlow or PyTorch.\n",
        "\n",
        "Available to work at least 3.5 days per week for a duration of 3-6 months.\n",
        "\n",
        "Proficient in English with the ability to effectively communicate and comprehend technical documents.\n",
        "\n",
        "Strong analytical and problem-solving skills.\n",
        "\n",
        "A collaborative mindset and a keen attention to detail with a commitment to producing high-quality work.\n",
        "\n",
        "Proactive, self-motivated, and capable of managing time effectively.\n",
        "\n",
        "Benefits and Perks\n",
        "Working in one of the Best Places to Work in Vietnam, Top 10 ITC Company in Vietnam\n",
        "\n",
        "Flexible working model: Flexible time & Hybrid working from Ho Chi Minh city\n",
        "\n",
        "Working and growing in a values-driven, international working environment and standard Agile culture with passionate and talented teams\n",
        "\n",
        "Various training on hot-trend technologies, best practices and soft skills\n",
        "\n",
        "Company trip, big annual year-end party every year, team building, etc.\n",
        "\n",
        "Fitness & sports activities: football, tennis, table tennis, badminton, yoga, swimming…\n",
        "\n",
        "Joining community development activities: 1% Pledge, charity every quarter, blood donation, public seminars, career orientation talks,…\n",
        "\n",
        "Free in-house entertainment facilities (football, ping pong, gym…), coffee, and snacks (instant noodles, cookies, candies…)\n",
        "\n",
        "Join us and explore other fantastic opportunities! To help us understand your competencies and match you with future suitable KMS ATIG opportunities, please send your academic transcript & the latest CV mentioning your desired position.\n",
        "\n",
        "\n",
        "Send your Resume including Academic Transcript to join us and let yourself explore other fantastic things!\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "KFdxepqx6eoy"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "job_description = \"\"\"\n",
        "We are looking for a Data Engineer to join our team. You will use various methods to transform raw data into useful data systems. Overall, you’ll strive for efficiency by aligning data systems with business goals. To succeed in this data engineering position, you should have strong analytical skills and the ability to combine data from different sources. Data engineer skills also include familiarity with several programming languages and knowledge of learning machine methods.\n",
        "\n",
        "Responsibilities:\n",
        "Analyze and organize raw data\n",
        "Build data systems and pipelines\n",
        "Evaluate business needs and objectives\n",
        "Interpret trends and patterns\n",
        "Prepare data for prescriptive and predictive modeling\n",
        "Combine raw information from different sources\n",
        "Explore ways to enhance data quality and reliability\n",
        "Identify opportunities for data acquisition\n",
        "Collaborate with data scientists and architects on several projects.\n",
        "English proficiency in written and spoken\n",
        "\n",
        "Yêu cầu\n",
        "At least 3 years of experience working as a Data Engineer role\n",
        "Technical expertise with data models, data mining, and segmentation techniques\n",
        "Knowledge of programming languages (e.g. Java, Python)\n",
        "Hands-on experience with SQL database design\n",
        "Great numerical and analytical skills\n",
        "Degree in Computer Science, IT, or similar field\n",
        "Excellent communication and presentation skills.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "T0GkpcZr_6C8"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "job_description = \"\"\"\n",
        "Full job description\n",
        "HCL Vietnam Company Limited\n",
        "\n",
        "\n",
        "Level 11, Five Star Tower, 28Bis Mac Dinh Chi Street, Da Kao Ward, District 1, Ho Chi Minh\n",
        "Leadvisors Tower, Level 15, 643 Pham Van Dong Street, Co Nhue Ward, Bac Tu Liem, Ha Noi\n",
        "Hybrid\n",
        "Posted 17 days ago\n",
        "Skills:\n",
        "Database\n",
        "\n",
        "Python\n",
        "\n",
        "Top 3 reasons to join us\n",
        "Attractive Salary and Performance Bonus\n",
        "Health Care for Employees & Family, 18 paid leaves\n",
        "Chance to travel onsite (in 53 countries)\n",
        "Job description\n",
        "INTRODUCTION\n",
        "HCL Technologies, as the leader in Cloud & Digital Analytics services, are proud to be the key technology partner for our customer in this digital transformation journey. HCL is setting up large delivery centers for digital transformation projects in Vietnam and as part of that we are starting a delivery center in Ho Chi Minh City for Data transformation initiative of a major Global Bank. Work involves transforming legacy data into modern digital data technology and hosting it in Google Cloud Platform.\n",
        "This is an exciting project in which employees get the opportunity to experience the most modern data tool stack. Project is delivered in Agile ways of working, an excellent opportunity to work for a global leader. Joining us, you are the ones who create this innovative journey, you will have many chances to learn & work with the latest Cloud and Data Analytics technologies.\n",
        "\n",
        "DUTY & RESPONSIBILITIES\n",
        "As a Hadoop Big Data Engineer, you will operate and monitor scalable and resilient data platform based on Hadoop ecosystem, on production phase\n",
        "\n",
        "Major Tasks (80%)\n",
        "Responsible for production environment, build, deploy, monitor and bug fix on production environment.\n",
        "Provide high operational excellence guaranteeing high availability and platform stability.\n",
        "Technical analysis, trouble shooting and fixing the Production Incidents, Problem tickets and\n",
        "Changes\n",
        "Take ownership of (Data processing/Batch Jobs) applications from a Support & Maintenance perspective\n",
        "Work on small enhancements (Analysis, Build/test, Deployment/Release support)\n",
        "\n",
        "Other Tasks (20%)\n",
        "Engineer reliable data pipelines for sourcing, processing, transforming, enriching and storing data in different ways, using data platform infrastructure effectively\n",
        "Ingest and transform data sets from a variety of data sources\n",
        "Focus on ingesting, storing, processing, and analyzing large datasets\n",
        "Create scalable, high-performance web services for tracking data\n",
        "Your skills and experience\n",
        "Must requirements:\n",
        "Must have 3+ years of experience at a similar role\n",
        "Having hands on experience in Hadoop ecosystem (on-prem) including Spark, HDFS, MapReduce, YARN, Hive, Kafka,\n",
        "Understanding of SLA and meeting Timelines for SRE activities\n",
        "Expertise in at least one commercial distribution of Hadoop\n",
        "Good experience in automating manual activities using python or unix scripts\n",
        "Good experience in log analysis and management in distributed Hadoop platform along with dashboarding and automated monitoring of applications health.\n",
        "Strong Linux/Unix skills\n",
        "Experience debugging and deploying application on Big Data platform\n",
        "\n",
        "Good to Have:\n",
        "Experience with Data warehouse and Data management: Data Quality, Data integration\n",
        "Experience in ETL, SQL and NoSQL Database\n",
        "Good in programming language Python, Java, etc\n",
        "Exposure to any CI/CD tools such as Jenkins, Bamboo or Github actions\n",
        "Experience in Data API\n",
        "Why you'll love working here\n",
        "18 paid leaves/year (12 annual leaves and 6 personal leaves)\n",
        "Insurance plan based on full salary + 13th month salary + Performance bonus\n",
        "Meal allowance of 730,000 VND/month\n",
        "100% full salary and benefits as an official employee from the 1st day of working\n",
        "Medical benefit for employee and family\n",
        "Working in a fast-paced, flexible, and multinational working environment. Chance to travel onsite (in 53 countries)\n",
        "Free snacks, refreshment, and parking\n",
        "Internal training (Technical & Functional & English)\n",
        "Working time: 8:30 AM - 6:00 PM from Mondays to Fridays\n",
        "\n",
        "HCL Vietnam Company Limited\n",
        "\n",
        "We bring together the best of technology and our people to supercharge progress.\n",
        "Company type\n",
        "IT Outsourcing\n",
        "Company size\n",
        "501-1000 employees\n",
        "Country\n",
        "India\n",
        "Working days\n",
        "Monday - Friday\n",
        "Overtime policy\n",
        "No OT\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "IQI6abliDiFP"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from typing import Dict, Any, List\n",
        "\n",
        "class Node(BaseModel):\n",
        "    name: str = Field(description=\"Name of entity, ex: job position, company name, skill, etc\")\n",
        "    entity_type: str = Field(description= \"Type of entity\")\n",
        "    attributes: Dict[str, str] = Field(description=\"Attribute of entity\")\n",
        "\n",
        "class Edge(BaseModel):\n",
        "    name: str = Field(description= \"Name of relationship between 2 entities\")\n",
        "    source: Node = Field(description= \"Source entity\")\n",
        "    target: Node = Field(description= \"Target entity\")\n",
        "\n",
        "class KnowledgeGraph(BaseModel):\n",
        "    nodes: List[Node] = Field(default_factory=list)\n",
        "    edges: List[Edge] = Field(default_factory=list)"
      ],
      "metadata": {
        "id": "BhSrrP077uCO"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import instructor\n",
        "from pydantic import BaseModel\n",
        "\n",
        "client = instructor.from_gemini(\n",
        "    client=genai.GenerativeModel(\n",
        "        model_name=\"models/gemini-1.5-flash-latest\", # model defaults to \"gemini-pro\"\n",
        "    ),\n",
        "    mode=instructor.Mode.GEMINI_JSON,\n",
        ")\n",
        "\n",
        "# note that client.chat.completions.create will also work\n",
        "resp = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"Help me understand the following by describing it as a detailed knowledge graph, you only extract entity in text: {job_description}\",\n",
        "        }\n",
        "    ],\n",
        "    response_model=KnowledgeGraph,\n",
        ")"
      ],
      "metadata": {
        "id": "dYDNV-484kuA"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Chl3anMs6Hvn",
        "outputId": "652fd11f-1647-45a6-e20a-4cb07095d3e6"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KnowledgeGraph(nodes=[Node(name='Hadoop Big Data Engineer', entity_type='Job', attributes={'description': 'operate and monitor scalable and resilient data platform based on Hadoop ecosystem, on production phase'}), Node(name='HCL Technologies', entity_type='Company', attributes={'description': 'leader in Cloud & Digital Analytics services'}), Node(name='Data transformation', entity_type='Project', attributes={'description': 'transforming legacy data into modern digital data technology and hosting it in Google Cloud Platform'}), Node(name='Global Bank', entity_type='Company', attributes={}), Node(name='Ho Chi Minh City', entity_type='Location', attributes={}), Node(name='Google Cloud Platform', entity_type='Platform', attributes={}), Node(name='Hadoop ecosystem', entity_type='Technology', attributes={}), Node(name='Spark', entity_type='Technology', attributes={}), Node(name='HDFS', entity_type='Technology', attributes={}), Node(name='MapReduce', entity_type='Technology', attributes={}), Node(name='YARN', entity_type='Technology', attributes={}), Node(name='Hive', entity_type='Technology', attributes={}), Node(name='Kafka', entity_type='Technology', attributes={}), Node(name='SLA', entity_type='Technology', attributes={}), Node(name='SRE', entity_type='Technology', attributes={}), Node(name='Hadoop', entity_type='Technology', attributes={}), Node(name='Python', entity_type='Technology', attributes={}), Node(name='Unix', entity_type='Technology', attributes={}), Node(name='Linux', entity_type='Technology', attributes={}), Node(name='Data warehouse', entity_type='Technology', attributes={}), Node(name='Data management', entity_type='Technology', attributes={}), Node(name='Data Quality', entity_type='Technology', attributes={}), Node(name='Data integration', entity_type='Technology', attributes={}), Node(name='ETL', entity_type='Technology', attributes={}), Node(name='SQL', entity_type='Technology', attributes={}), Node(name='NoSQL Database', entity_type='Technology', attributes={}), Node(name='Java', entity_type='Technology', attributes={}), Node(name='Jenkins', entity_type='Technology', attributes={}), Node(name='Bamboo', entity_type='Technology', attributes={}), Node(name='Github actions', entity_type='Technology', attributes={}), Node(name='Data API', entity_type='Technology', attributes={}), Node(name='IT Outsourcing', entity_type='Industry', attributes={}), Node(name='India', entity_type='Country', attributes={}), Node(name='Vietnam', entity_type='Country', attributes={})], edges=[Edge(name='WORKS_FOR', source=Node(name='Hadoop Big Data Engineer', entity_type='Job', attributes={'description': 'operate and monitor scalable and resilient data platform based on Hadoop ecosystem, on production phase'}), target=Node(name='HCL Technologies', entity_type='Company', attributes={'description': 'leader in Cloud & Digital Analytics services'})), Edge(name='LOCATED_IN', source=Node(name='Ho Chi Minh City', entity_type='Location', attributes={}), target=Node(name='Vietnam', entity_type='Country', attributes={})), Edge(name='WORKS_ON', source=Node(name='Hadoop Big Data Engineer', entity_type='Job', attributes={'description': 'operate and monitor scalable and resilient data platform based on Hadoop ecosystem, on production phase'}), target=Node(name='Data transformation', entity_type='Project', attributes={'description': 'transforming legacy data into modern digital data technology and hosting it in Google Cloud Platform'})), Edge(name='PART_OF', source=Node(name='Data transformation', entity_type='Project', attributes={'description': 'transforming legacy data into modern digital data technology and hosting it in Google Cloud Platform'}), target=Node(name='Global Bank', entity_type='Company', attributes={})), Edge(name='HOSTED_ON', source=Node(name='Data transformation', entity_type='Project', attributes={'description': 'transforming legacy data into modern digital data technology and hosting it in Google Cloud Platform'}), target=Node(name='Google Cloud Platform', entity_type='Platform', attributes={})), Edge(name='USES', source=Node(name='Hadoop Big Data Engineer', entity_type='Job', attributes={'description': 'operate and monitor scalable and resilient data platform based on Hadoop ecosystem, on production phase'}), target=Node(name='Hadoop ecosystem', entity_type='Technology', attributes={})), Edge(name='USES', source=Node(name='Hadoop Big Data Engineer', entity_type='Job', attributes={'description': 'operate and monitor scalable and resilient data platform based on Hadoop ecosystem, on production phase'}), target=Node(name='Spark', entity_type='Technology', attributes={})), Edge(name='USES', source=Node(name='Hadoop Big Data Engineer', entity_type='Job', attributes={'description': 'operate and monitor scalable and resilient data platform based on Hadoop ecosystem, on production phase'}), target=Node(name='HDFS', entity_type='Technology', attributes={})), Edge(name='USES', source=Node(name='Hadoop Big Data Engineer', entity_type='Job', attributes={'description': 'operate and monitor scalable and resilient data platform based on Hadoop ecosystem, on production phase'}), target=Node(name='MapReduce', entity_type='Technology', attributes={})), Edge(name='USES', source=Node(name='Hadoop Big Data Engineer', entity_type='Job', attributes={'description': 'operate and monitor scalable and resilient data platform based on Hadoop ecosystem, on production phase'}), target=Node(name='YARN', entity_type='Technology', attributes={})), Edge(name='USES', source=Node(name='Hadoop Big Data Engineer', entity_type='Job', attributes={'description': 'operate and monitor scalable and resilient data platform based on Hadoop ecosystem, on production phase'}), target=Node(name='Hive', entity_type='Technology', attributes={})), Edge(name='USES', source=Node(name='Hadoop Big Data Engineer', entity_type='Job', attributes={'description': 'operate and monitor scalable and resilient data platform based on Hadoop ecosystem, on production phase'}), target=Node(name='Kafka', entity_type='Technology', attributes={})), Edge(name='USES', source=Node(name='Hadoop Big Data Engineer', entity_type='Job', attributes={'description': 'operate and monitor scalable and resilient data platform based on Hadoop ecosystem, on production phase'}), target=Node(name='SLA', entity_type='Technology', attributes={})), Edge(name='USES', source=Node(name='Hadoop Big Data Engineer', entity_type='Job', attributes={'description': 'operate and monitor scalable and resilient data platform based on Hadoop ecosystem, on production phase'}), target=Node(name='SRE', entity_type='Technology', attributes={})), Edge(name='USES', source=Node(name='Hadoop Big Data Engineer', entity_type='Job', attributes={'description': 'operate and monitor scalable and resilient data platform based on Hadoop ecosystem, on production phase'}), target=Node(name='Hadoop', entity_type='Technology', attributes={})), Edge(name='USES', source=Node(name='Hadoop Big Data Engineer', entity_type='Job', attributes={'description': 'operate and monitor scalable and resilient data platform based on Hadoop ecosystem, on production phase'}), target=Node(name='Python', entity_type='Technology', attributes={})), Edge(name='USES', source=Node(name='Hadoop Big Data Engineer', entity_type='Job', attributes={'description': 'operate and monitor scalable and resilient data platform based on Hadoop ecosystem, on production phase'}), target=Node(name='Unix', entity_type='Technology', attributes={})), Edge(name='USES', source=Node(name='Hadoop Big Data Engineer', entity_type='Job', attributes={'description': 'operate and monitor scalable and resilient data platform based on Hadoop ecosystem, on production phase'}), target=Node(name='Linux', entity_type='Technology', attributes={})), Edge(name='USES', source=Node(name='Hadoop Big Data Engineer', entity_type='Job', attributes={'description': 'operate and monitor scalable and resilient data platform based on Hadoop ecosystem, on production phase'}), target=Node(name='Data warehouse', entity_type='Technology', attributes={})), Edge(name='USES', source=Node(name='Hadoop Big Data Engineer', entity_type='Job', attributes={'description': 'operate and monitor scalable and resilient data platform based on Hadoop ecosystem, on production phase'}), target=Node(name='Data management', entity_type='Technology', attributes={})), Edge(name='USES', source=Node(name='Hadoop Big Data Engineer', entity_type='Job', attributes={'description': 'operate and monitor scalable and resilient data platform based on Hadoop ecosystem, on production phase'}), target=Node(name='Data Quality', entity_type='Technology', attributes={})), Edge(name='USES', source=Node(name='Hadoop Big Data Engineer', entity_type='Job', attributes={'description': 'operate and monitor scalable and resilient data platform based on Hadoop ecosystem, on production phase'}), target=Node(name='Data integration', entity_type='Technology', attributes={})), Edge(name='USES', source=Node(name='Hadoop Big Data Engineer', entity_type='Job', attributes={'description': 'operate and monitor scalable and resilient data platform based on Hadoop ecosystem, on production phase'}), target=Node(name='ETL', entity_type='Technology', attributes={})), Edge(name='USES', source=Node(name='Hadoop Big Data Engineer', entity_type='Job', attributes={'description': 'operate and monitor scalable and resilient data platform based on Hadoop ecosystem, on production phase'}), target=Node(name='SQL', entity_type='Technology', attributes={})), Edge(name='USES', source=Node(name='Hadoop Big Data Engineer', entity_type='Job', attributes={'description': 'operate and monitor scalable and resilient data platform based on Hadoop ecosystem, on production phase'}), target=Node(name='NoSQL Database', entity_type='Technology', attributes={})), Edge(name='USES', source=Node(name='Hadoop Big Data Engineer', entity_type='Job', attributes={'description': 'operate and monitor scalable and resilient data platform based on Hadoop ecosystem, on production phase'}), target=Node(name='Java', entity_type='Technology', attributes={})), Edge(name='USES', source=Node(name='Hadoop Big Data Engineer', entity_type='Job', attributes={'description': 'operate and monitor scalable and resilient data platform based on Hadoop ecosystem, on production phase'}), target=Node(name='Jenkins', entity_type='Technology', attributes={})), Edge(name='USES', source=Node(name='Hadoop Big Data Engineer', entity_type='Job', attributes={'description': 'operate and monitor scalable and resilient data platform based on Hadoop ecosystem, on production phase'}), target=Node(name='Bamboo', entity_type='Technology', attributes={})), Edge(name='USES', source=Node(name='Hadoop Big Data Engineer', entity_type='Job', attributes={'description': 'operate and monitor scalable and resilient data platform based on Hadoop ecosystem, on production phase'}), target=Node(name='Github actions', entity_type='Technology', attributes={})), Edge(name='USES', source=Node(name='Hadoop Big Data Engineer', entity_type='Job', attributes={'description': 'operate and monitor scalable and resilient data platform based on Hadoop ecosystem, on production phase'}), target=Node(name='Data API', entity_type='Technology', attributes={})), Edge(name='IS_IN', source=Node(name='HCL Technologies', entity_type='Company', attributes={'description': 'leader in Cloud & Digital Analytics services'}), target=Node(name='IT Outsourcing', entity_type='Industry', attributes={})), Edge(name='LOCATED_IN', source=Node(name='HCL Technologies', entity_type='Company', attributes={'description': 'leader in Cloud & Digital Analytics services'}), target=Node(name='India', entity_type='Country', attributes={})), Edge(name='LOCATED_IN', source=Node(name='HCL Technologies', entity_type='Company', attributes={'description': 'leader in Cloud & Digital Analytics services'}), target=Node(name='Vietnam', entity_type='Country', attributes={}))])"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for node in resp.nodes:\n",
        "    print(node.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3A2veff75fN",
        "outputId": "d12289c3-31d6-460d-f1f5-c919c933d3a4"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hadoop Big Data Engineer\n",
            "HCL Technologies\n",
            "Data transformation\n",
            "Global Bank\n",
            "Ho Chi Minh City\n",
            "Google Cloud Platform\n",
            "Hadoop ecosystem\n",
            "Spark\n",
            "HDFS\n",
            "MapReduce\n",
            "YARN\n",
            "Hive\n",
            "Kafka\n",
            "SLA\n",
            "SRE\n",
            "Hadoop\n",
            "Python\n",
            "Unix\n",
            "Linux\n",
            "Data warehouse\n",
            "Data management\n",
            "Data Quality\n",
            "Data integration\n",
            "ETL\n",
            "SQL\n",
            "NoSQL Database\n",
            "Java\n",
            "Jenkins\n",
            "Bamboo\n",
            "Github actions\n",
            "Data API\n",
            "IT Outsourcing\n",
            "India\n",
            "Vietnam\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for r in resp.edges:\n",
        "    print(f\"{r.source.name} => {r.name} => {r.target.name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umOd1Wyx9rQg",
        "outputId": "b8374878-8c86-47e8-ccc0-e9f4b334c0ae"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hadoop Big Data Engineer => WORKS_FOR => HCL Technologies\n",
            "Ho Chi Minh City => LOCATED_IN => Vietnam\n",
            "Hadoop Big Data Engineer => WORKS_ON => Data transformation\n",
            "Data transformation => PART_OF => Global Bank\n",
            "Data transformation => HOSTED_ON => Google Cloud Platform\n",
            "Hadoop Big Data Engineer => USES => Hadoop ecosystem\n",
            "Hadoop Big Data Engineer => USES => Spark\n",
            "Hadoop Big Data Engineer => USES => HDFS\n",
            "Hadoop Big Data Engineer => USES => MapReduce\n",
            "Hadoop Big Data Engineer => USES => YARN\n",
            "Hadoop Big Data Engineer => USES => Hive\n",
            "Hadoop Big Data Engineer => USES => Kafka\n",
            "Hadoop Big Data Engineer => USES => SLA\n",
            "Hadoop Big Data Engineer => USES => SRE\n",
            "Hadoop Big Data Engineer => USES => Hadoop\n",
            "Hadoop Big Data Engineer => USES => Python\n",
            "Hadoop Big Data Engineer => USES => Unix\n",
            "Hadoop Big Data Engineer => USES => Linux\n",
            "Hadoop Big Data Engineer => USES => Data warehouse\n",
            "Hadoop Big Data Engineer => USES => Data management\n",
            "Hadoop Big Data Engineer => USES => Data Quality\n",
            "Hadoop Big Data Engineer => USES => Data integration\n",
            "Hadoop Big Data Engineer => USES => ETL\n",
            "Hadoop Big Data Engineer => USES => SQL\n",
            "Hadoop Big Data Engineer => USES => NoSQL Database\n",
            "Hadoop Big Data Engineer => USES => Java\n",
            "Hadoop Big Data Engineer => USES => Jenkins\n",
            "Hadoop Big Data Engineer => USES => Bamboo\n",
            "Hadoop Big Data Engineer => USES => Github actions\n",
            "Hadoop Big Data Engineer => USES => Data API\n",
            "HCL Technologies => IS_IN => IT Outsourcing\n",
            "HCL Technologies => LOCATED_IN => India\n",
            "HCL Technologies => LOCATED_IN => Vietnam\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "entity_type = []\n",
        "for node in resp.nodes:\n",
        "    entity_type.append(node.entity_type)\n",
        "\n",
        "set(entity_type)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GIcLo1v-A5K",
        "outputId": "c83777d4-6682-4db2-f844-0a55485fb63f"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Company',\n",
              " 'Country',\n",
              " 'Industry',\n",
              " 'Job',\n",
              " 'Location',\n",
              " 'Platform',\n",
              " 'Project',\n",
              " 'Technology'}"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Write Cypher to create Knowledge Graph"
      ],
      "metadata": {
        "id": "SudnOaFDIzVg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Connect to Neo4j Server"
      ],
      "metadata": {
        "id": "X33GJm9BJ4vk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_community.graphs import Neo4jGraph\n",
        "\n",
        "os.environ[\"NEO4J_URI\"] = \"neo4j+s://ae9172fc.databases.neo4j.io\"\n",
        "os.environ[\"NEO4J_USERNAME\"] = \"neo4j\"\n",
        "os.environ[\"NEO4J_PASSWORD\"] = \"J4at6oDaPMqkYjycCKyXZglSnU4OsW0AupVqP8Dlvdw\"\n",
        "\n",
        "knowledge_graph = Neo4jGraph()"
      ],
      "metadata": {
        "id": "S4L5XfrhIkrB"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(knowledge_graph.schema)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToWZV0LYLuaC",
        "outputId": "f5926f47-50f8-4f00-d5b0-9d7d9db577c5"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node properties:\n",
            "Country {name: STRING}\n",
            "Company {name: STRING, founded_year: INTEGER, location: STRING, development_centers: STRING, service: STRING, expertise: STRING}\n",
            "Business_Line {name: STRING, focus: STRING, benefit: STRING}\n",
            "Department {name: STRING, focus: STRING, benefit: STRING}\n",
            "Job {name: STRING, location: STRING, responsibility: STRING}\n",
            "Technology {name: STRING}\n",
            "Academic_Field {name: STRING}\n",
            "Framework {name: STRING}\n",
            "City {name: STRING}\n",
            "Relationship properties:\n",
            "\n",
            "The relationships:\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Delete relationship"
      ],
      "metadata": {
        "id": "9oirz22RMVcr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cypher = \"\"\"\n",
        "# MATCH (m)-[r]->(n)\n",
        "# DELETE r\n",
        "# \"\"\"\n",
        "\n",
        "# knowledge_graph.query(cypher)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ONvmaQXL-y2",
        "outputId": "c79e6161-92e3-464b-c3f9-b953293132cf"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Delete node"
      ],
      "metadata": {
        "id": "zfDMfv9RMXfD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cypher = \"\"\"\n",
        "# MATCH (n)\n",
        "# DELETE n\n",
        "# \"\"\"\n",
        "\n",
        "# knowledge_graph.query(cypher)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwVTvZ-bMU_L",
        "outputId": "8e6c9a7a-1a35-4a8b-a3c8-1cd26398d1fe"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cypher = \"\"\"\n",
        "MATCH (n)\n",
        "RETURN n\n",
        "\"\"\"\n",
        "\n",
        "knowledge_graph.query(cypher)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbp0Z7p3MLre",
        "outputId": "09dfa7e9-aeca-43f9-ea6e-8136ac18b1e2"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'n': {'name': 'Opportunities'}},\n",
              " {'n': {'name': 'Data acquisition'}},\n",
              " {'n': {'name': 'Collaborate'}},\n",
              " {'n': {'name': 'Data scientists'}},\n",
              " {'n': {'name': 'Architects'}},\n",
              " {'n': {'name': 'Projects'}},\n",
              " {'n': {'form': 'Written and spoken',\n",
              "   'name': 'English proficiency',\n",
              "   'language': 'English'}},\n",
              " {'n': {'duration': '3 years', 'role': 'Data Engineer', 'name': 'Experience'}},\n",
              " {'n': {'name': 'Technical expertise',\n",
              "   'fields': 'Data models, data mining, and segmentation techniques'}},\n",
              " {'n': {'languages': 'Java, Python', 'name': 'Knowledge'}},\n",
              " {'n': {'field': 'SQL database design', 'name': 'Hands-on experience'}},\n",
              " {'n': {'name': 'Numerical skills'}},\n",
              " {'n': {'name': 'Analytical skills'}},\n",
              " {'n': {'field': 'Computer Science, IT, or similar field', 'name': 'Degree'}},\n",
              " {'n': {'form': 'Excellent', 'name': 'Communication skills'}},\n",
              " {'n': {'name': 'Reliability'}},\n",
              " {'n': {'name': 'Identify'}},\n",
              " {'n': {'development_centers': 'Vietnam, Mexico',\n",
              "   'name': 'KMS Technology',\n",
              "   'founded': 2009,\n",
              "   'location': 'US',\n",
              "   'services': 'Software Consulting, Software Development, Technology Solutions',\n",
              "   'expertise': 'Software Engineering'}},\n",
              " {'n': {'solutions': 'Innovative Technology Solutions',\n",
              "   'name': 'KMS Software',\n",
              "   'focus': 'Software Domain Expertise',\n",
              "   'services': 'Technology Platforms, Speed-to-Market, Development Support'}},\n",
              " {'n': {'solutions': 'Digitalization, Technology Consulting, Data Analytics, Software Development, Software Quality',\n",
              "   'name': 'KMS Solutions',\n",
              "   'focus': 'BFSI Businesses',\n",
              "   'services': 'Digital Finance Revolution, Technology Consulting, Data Analytics, Software Development, Software Quality'}},\n",
              " {'n': {'solutions': 'Transformative Next-gen Technologies',\n",
              "   'target_audience': 'Providers, Payers, Life Sciences, Medical Technology Vendors',\n",
              "   'name': 'KMS Healthcare',\n",
              "   'focus': 'Healthcare',\n",
              "   'services': 'Transformative Next-gen Technologies, Innovative Tools, Expertise'}},\n",
              " {'n': {'hub': 'Creativity, Technological Advancement',\n",
              "   'name': 'KMS Innovation Center',\n",
              "   'focus': 'Pushing Boundaries of Technology',\n",
              "   'areas': 'AI, Software Development',\n",
              "   'opportunities': 'AI/ML Engineer Internship',\n",
              "   'goals': 'Revolutionary Solutions, Anticipating and Exceeding Industry Needs'}},\n",
              " {'n': {'benefits': 'Working in one of the Best Places to Work in Vietnam, Top 10 ITC Company in Vietnam, Flexible working model, International working environment, Standard Agile culture, Training on hot-trend technologies, Company trip, Annual year-end party, Team building, Fitness & sports activities, Community development activities, Free in-house entertainment facilities, Coffee and snacks',\n",
              "   'requirements': 'Pursuing a degree in Computer Science, AI/ML, or a related field, Solid understanding of NLP, Deep Learning, and Generative AI, Practical experience in machine learning frameworks such as TensorFlow or PyTorch, Available to work at least 3.5 days per week for a duration of 3-6 months, Proficient in English, Strong analytical and problem-solving skills, Collaborative mindset and keen attention to detail, Proactive, self-motivated, and capable of managing time effectively',\n",
              "   'responsibilities': 'Explore and evaluate advancements in AI models and emerging technologies, Build proofs of concept for AI applications and solutions, Collaborate with technical leaders and developers, Contribute to innovative solutions tailored to customer needs, Engage in continuous learning activities',\n",
              "   'name': 'AI/ML Engineer Internship',\n",
              "   'focus': 'AI, Emerging Technologies',\n",
              "   'department': 'KMS Innovation Center'}},\n",
              " {'n': {'name': 'Vietnam'}},\n",
              " {'n': {'name': 'Mexico'}},\n",
              " {'n': {'name': 'US'}},\n",
              " {'n': {'name': 'TensorFlow'}},\n",
              " {'n': {'name': 'PyTorch'}},\n",
              " {'n': {'name': 'NLP'}},\n",
              " {'n': {'name': 'Deep Learning'}},\n",
              " {'n': {'name': 'Generative AI'}},\n",
              " {'n': {'name': 'Computer Science'}},\n",
              " {'n': {'name': 'AI/ML'}},\n",
              " {'n': {'name': 'English'}},\n",
              " {'n': {'name': 'AI'}},\n",
              " {'n': {'name': 'Machine Learning'}},\n",
              " {'n': {'name': 'Emerging Technologies'}},\n",
              " {'n': {'name': 'Software Development'}},\n",
              " {'n': {'name': 'Data Analytics'}},\n",
              " {'n': {'skills': 'Strong analytical skills',\n",
              "   'familiarity': 'Familiarity with several programming languages',\n",
              "   'responsibilities': 'Transform raw data into useful data systems',\n",
              "   'goal': 'Efficiency',\n",
              "   'name': 'Data Engineer',\n",
              "   'team': 'Our team',\n",
              "   'ability': 'Ability to combine data from different sources',\n",
              "   'alignment': 'Aligning data systems with business goals',\n",
              "   'knowledge': 'Knowledge of learning machine methods'}},\n",
              " {'n': {'name': 'Raw data'}},\n",
              " {'n': {'name': 'Useful data systems'}},\n",
              " {'n': {'name': 'Business goals'}},\n",
              " {'n': {'name': 'Data systems'}},\n",
              " {'n': {'name': 'Analytical skills'}},\n",
              " {'n': {'name': 'Data sources'}},\n",
              " {'n': {'name': 'Programming languages'}},\n",
              " {'n': {'name': 'Learning machine methods'}},\n",
              " {'n': {'name': 'Analyze'}},\n",
              " {'n': {'name': 'Organize'}},\n",
              " {'n': {'name': 'Build'}},\n",
              " {'n': {'name': 'Pipelines'}},\n",
              " {'n': {'name': 'Evaluate'}},\n",
              " {'n': {'name': 'Business needs'}},\n",
              " {'n': {'name': 'Objectives'}},\n",
              " {'n': {'name': 'Interpret'}},\n",
              " {'n': {'name': 'Trends'}},\n",
              " {'n': {'name': 'Patterns'}},\n",
              " {'n': {'name': 'Prepare'}},\n",
              " {'n': {'name': 'Data'}},\n",
              " {'n': {'name': 'Prescriptive modeling'}},\n",
              " {'n': {'name': 'Predictive modeling'}},\n",
              " {'n': {'name': 'Combine'}},\n",
              " {'n': {'name': 'Raw information'}},\n",
              " {'n': {'name': 'Different sources'}},\n",
              " {'n': {'name': 'Explore'}},\n",
              " {'n': {'name': 'Ways'}},\n",
              " {'n': {'name': 'Enhance'}},\n",
              " {'n': {'name': 'Data quality'}},\n",
              " {'n': {'form': 'Excellent', 'name': 'Presentation skills'}}]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for node in resp.nodes:\n",
        "    name = node.name\n",
        "    entity_type = \"_\".join(node.entity_type.split())\n",
        "    attribute = node.attributes\n",
        "\n",
        "    # Create nodes\n",
        "    cypher = f\"\"\"\n",
        "    CREATE (ex:{entity_type} {{name: \"{name}\"}})\n",
        "    \"\"\"\n",
        "    knowledge_graph.query(cypher)\n",
        "\n",
        "    # Add attributes for node\n",
        "    for atr_name, atr_val in attribute.items():\n",
        "        try:\n",
        "            atr_val = int(atr_val)\n",
        "            cypher = f\"\"\"\n",
        "                MATCH (ex:{entity_type} {{name: \"{name}\"}})\n",
        "                SET ex.{atr_name} = {atr_val}\n",
        "            \"\"\"\n",
        "        except ValueError:\n",
        "            cypher = f\"\"\"\n",
        "                MATCH (ex:{entity_type} {{name: \"{name}\"}})\n",
        "                SET ex.{atr_name} = \"{atr_val}\"\n",
        "            \"\"\"\n",
        "\n",
        "        knowledge_graph.query(cypher)"
      ],
      "metadata": {
        "id": "ilSBYHdzJ7fX"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for relation in resp.edges:\n",
        "    source = relation.source\n",
        "    target = relation.target\n",
        "    relation_name = \"_\".join(relation.name.split())\n",
        "\n",
        "    source_type = \"_\".join(source.entity_type.split())\n",
        "    target_type = \"_\".join(target.entity_type.split())\n",
        "\n",
        "    cypher = f\"\"\"\n",
        "    MATCH (source:{source_type} {{name: \"{source.name}\"}}), (target:{target_type} {{name: \"{target.name}\"}})\n",
        "    CREATE (source)-[:{relation_name}]->(target)\n",
        "    \"\"\"\n",
        "\n",
        "    knowledge_graph.query(cypher)"
      ],
      "metadata": {
        "id": "Mrxb6RCZTCia"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3obPpRyo-eYb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}